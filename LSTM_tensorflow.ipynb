{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_eq4OTfjOFb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LwZ6gjrKGtjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = open('/content/HP1_paragraph.txt').read().lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4buobLqmMvn",
        "outputId": "fb35e7fc-6578-4f42-e979-9463d7288306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_count(paragraph):\n",
        "  word_count = 0\n",
        "  for i in range(len(paragraph)):\n",
        "      if paragraph[i] == ' ' or paragraph[i] == '\\n':\n",
        "        word_count += 1\n",
        "  return word_count"
      ],
      "metadata": {
        "id": "Y55wsIq46PhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = set(data)\n",
        "data_size, char_size = len(data), len(chars)\n",
        "print(f'Data size: {data_size}, Char Size: {char_size}')\n",
        "print(data[:10])\n",
        "word_count_data = word_count(data)\n",
        "print(f'word_count_data = {word_count_data}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNRN44gEmwZj",
        "outputId": "cf994404-8c7f-405b-9113-fbeeb82d293a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size: 1545, Char Size: 31\n",
            "harry pott\n",
            "word_count_data = 283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_index = int(0.8 * len(data))\n",
        "\n",
        "while data[split_index] != ' ' and data[split_index] != '\\n':\n",
        "  split_index += 1\n",
        "\n",
        "train_data = data[:split_index]\n",
        "test_data = data[split_index:]\n",
        "\n",
        "print(f'Training data size: {len(train_data)}, Training char size: {len(set(train_data))}')\n",
        "print(f'Test data size: {len(test_data)}, Test char size: {len(set(test_data))}')\n",
        "\n",
        "word_count_train = word_count(train_data)\n",
        "word_count_test = word_count(test_data)\n",
        "print(f'word_count_train = {word_count_train}')\n",
        "print(f'word_count_test = {word_count_test}')\n",
        "\n",
        "print(f'train_data = {train_data}')\n",
        "print(f'test_data = {test_data}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkV9i0-knNDT",
        "outputId": "cd8a4d69-d447-455e-e11d-7a99730724b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size: 1236, Training char size: 31\n",
            "Test data size: 309, Test char size: 28\n",
            "word_count_train = 226\n",
            "word_count_test = 57\n",
            "train_data = harry potter and the sorcerer's stone \n",
            "\n",
            "chapter one \n",
            "\n",
            "the boy who lived \n",
            "\n",
            "mr. and mrs. dursley, of number four, privet drive, were proud to say that they were perfectly normal, thank you very much. they were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. \n",
            "\n",
            "mr. dursley was the director of a firm called grunnings, which made drills. he was a big, beefy man with hardly any neck, although he did have a very large mustache. mrs. dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. the dursleys had a small son called dudley and in their opinion there was no finer boy anywhere. \n",
            "\n",
            "the dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. they didn't think they could bear it if anyone found out about the potters. mrs. potter was mrs. dursley's sister, but they hadn't met for several years; in fact, mrs. dursley pretended she didn't have a sister, because her sister and her good-for-nothing husband were as undursleyish as it was possible to be.\n",
            "test_data =  the dursleys shuddered to think what the neighbors would say if the potters arrived in the street. the dursleys knew that the potters had a small son, too, but they had never even seen him. this boy was another good reason for keeping the potters away; they didn't want dudley mixing with a child like that. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample paragraph\n",
        "paragraph = train_data\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([paragraph])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in paragraph.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Create predictors and label\n",
        "X, y = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit model\n",
        "model.fit(X, y, epochs=100, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dddu1wWdnZz7",
        "outputId": "832b8347-a96f-49a0-e097-173ed2de4aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "7/7 [==============================] - 4s 118ms/step - loss: 4.9195 - accuracy: 0.0188\n",
            "Epoch 2/100\n",
            "7/7 [==============================] - 1s 106ms/step - loss: 4.8998 - accuracy: 0.0469\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 4.8385 - accuracy: 0.0516\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 4.7608 - accuracy: 0.0329\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 4.7168 - accuracy: 0.0329\n",
            "Epoch 6/100\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 4.6617 - accuracy: 0.0423\n",
            "Epoch 7/100\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 4.6191 - accuracy: 0.0376\n",
            "Epoch 8/100\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 4.5743 - accuracy: 0.0469\n",
            "Epoch 9/100\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 4.5195 - accuracy: 0.0469\n",
            "Epoch 10/100\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 4.4497 - accuracy: 0.0563\n",
            "Epoch 11/100\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 4.3724 - accuracy: 0.0563\n",
            "Epoch 12/100\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 4.2897 - accuracy: 0.0751\n",
            "Epoch 13/100\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 4.2028 - accuracy: 0.0939\n",
            "Epoch 14/100\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 4.1107 - accuracy: 0.0798\n",
            "Epoch 15/100\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 4.0055 - accuracy: 0.0845\n",
            "Epoch 16/100\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 3.9080 - accuracy: 0.0892\n",
            "Epoch 17/100\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 3.7934 - accuracy: 0.1174\n",
            "Epoch 18/100\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 3.6926 - accuracy: 0.1033\n",
            "Epoch 19/100\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 3.6000 - accuracy: 0.1174\n",
            "Epoch 20/100\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 3.4839 - accuracy: 0.1221\n",
            "Epoch 21/100\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 3.4067 - accuracy: 0.1127\n",
            "Epoch 22/100\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 3.2958 - accuracy: 0.1268\n",
            "Epoch 23/100\n",
            "7/7 [==============================] - 1s 75ms/step - loss: 3.1881 - accuracy: 0.1455\n",
            "Epoch 24/100\n",
            "7/7 [==============================] - 1s 99ms/step - loss: 3.0899 - accuracy: 0.1737\n",
            "Epoch 25/100\n",
            "7/7 [==============================] - 1s 107ms/step - loss: 2.9979 - accuracy: 0.1784\n",
            "Epoch 26/100\n",
            "7/7 [==============================] - 1s 113ms/step - loss: 2.9190 - accuracy: 0.2066\n",
            "Epoch 27/100\n",
            "7/7 [==============================] - 1s 112ms/step - loss: 2.8318 - accuracy: 0.2113\n",
            "Epoch 28/100\n",
            "7/7 [==============================] - 1s 115ms/step - loss: 2.7562 - accuracy: 0.2254\n",
            "Epoch 29/100\n",
            "7/7 [==============================] - 1s 80ms/step - loss: 2.6970 - accuracy: 0.2207\n",
            "Epoch 30/100\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 2.6177 - accuracy: 0.2535\n",
            "Epoch 31/100\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 2.5427 - accuracy: 0.2864\n",
            "Epoch 32/100\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 2.4737 - accuracy: 0.3052\n",
            "Epoch 33/100\n",
            "7/7 [==============================] - 1s 98ms/step - loss: 2.4120 - accuracy: 0.3333\n",
            "Epoch 34/100\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 2.3513 - accuracy: 0.3756\n",
            "Epoch 35/100\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 2.2903 - accuracy: 0.3944\n",
            "Epoch 36/100\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 2.2416 - accuracy: 0.4038\n",
            "Epoch 37/100\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 2.1889 - accuracy: 0.4507\n",
            "Epoch 38/100\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 2.1508 - accuracy: 0.4366\n",
            "Epoch 39/100\n",
            "7/7 [==============================] - 0s 61ms/step - loss: 2.1005 - accuracy: 0.4883\n",
            "Epoch 40/100\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 2.0587 - accuracy: 0.4930\n",
            "Epoch 41/100\n",
            "7/7 [==============================] - 0s 61ms/step - loss: 2.0179 - accuracy: 0.5211\n",
            "Epoch 42/100\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 1.9748 - accuracy: 0.5070\n",
            "Epoch 43/100\n",
            "7/7 [==============================] - 1s 70ms/step - loss: 1.9367 - accuracy: 0.5775\n",
            "Epoch 44/100\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 1.8971 - accuracy: 0.5681\n",
            "Epoch 45/100\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 1.8587 - accuracy: 0.5962\n",
            "Epoch 46/100\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 1.8266 - accuracy: 0.6244\n",
            "Epoch 47/100\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 1.7875 - accuracy: 0.6854\n",
            "Epoch 48/100\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 1.7591 - accuracy: 0.6479\n",
            "Epoch 49/100\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 1.7225 - accuracy: 0.6291\n",
            "Epoch 50/100\n",
            "7/7 [==============================] - 1s 92ms/step - loss: 1.6944 - accuracy: 0.6901\n",
            "Epoch 51/100\n",
            "7/7 [==============================] - 1s 98ms/step - loss: 1.6686 - accuracy: 0.6854\n",
            "Epoch 52/100\n",
            "7/7 [==============================] - 1s 106ms/step - loss: 1.6386 - accuracy: 0.7042\n",
            "Epoch 53/100\n",
            "7/7 [==============================] - 1s 102ms/step - loss: 1.5988 - accuracy: 0.7230\n",
            "Epoch 54/100\n",
            "7/7 [==============================] - 1s 102ms/step - loss: 1.5685 - accuracy: 0.7418\n",
            "Epoch 55/100\n",
            "7/7 [==============================] - 1s 117ms/step - loss: 1.5501 - accuracy: 0.7559\n",
            "Epoch 56/100\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 1.5200 - accuracy: 0.8028\n",
            "Epoch 57/100\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 1.4883 - accuracy: 0.8075\n",
            "Epoch 58/100\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 1.4540 - accuracy: 0.8216\n",
            "Epoch 59/100\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 1.4321 - accuracy: 0.8357\n",
            "Epoch 60/100\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 1.4033 - accuracy: 0.8404\n",
            "Epoch 61/100\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 1.3784 - accuracy: 0.8310\n",
            "Epoch 62/100\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 1.3537 - accuracy: 0.8451\n",
            "Epoch 63/100\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 1.3374 - accuracy: 0.8545\n",
            "Epoch 64/100\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 1.3140 - accuracy: 0.8498\n",
            "Epoch 65/100\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 1.2854 - accuracy: 0.8404\n",
            "Epoch 66/100\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 1.2670 - accuracy: 0.8685\n",
            "Epoch 67/100\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 1.2312 - accuracy: 0.8920\n",
            "Epoch 68/100\n",
            "7/7 [==============================] - 0s 61ms/step - loss: 1.2066 - accuracy: 0.8920\n",
            "Epoch 69/100\n",
            "7/7 [==============================] - 0s 71ms/step - loss: 1.1788 - accuracy: 0.9108\n",
            "Epoch 70/100\n",
            "7/7 [==============================] - 0s 66ms/step - loss: 1.1583 - accuracy: 0.9155\n",
            "Epoch 71/100\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 1.1333 - accuracy: 0.9155\n",
            "Epoch 72/100\n",
            "7/7 [==============================] - 1s 94ms/step - loss: 1.1128 - accuracy: 0.9202\n",
            "Epoch 73/100\n",
            "7/7 [==============================] - 1s 76ms/step - loss: 1.0965 - accuracy: 0.9202\n",
            "Epoch 74/100\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 1.0773 - accuracy: 0.9296\n",
            "Epoch 75/100\n",
            "7/7 [==============================] - 0s 72ms/step - loss: 1.0501 - accuracy: 0.9437\n",
            "Epoch 76/100\n",
            "7/7 [==============================] - 1s 99ms/step - loss: 1.0326 - accuracy: 0.9531\n",
            "Epoch 77/100\n",
            "7/7 [==============================] - 1s 104ms/step - loss: 1.0216 - accuracy: 0.9437\n",
            "Epoch 78/100\n",
            "7/7 [==============================] - 1s 100ms/step - loss: 0.9926 - accuracy: 0.9390\n",
            "Epoch 79/100\n",
            "7/7 [==============================] - 1s 98ms/step - loss: 0.9690 - accuracy: 0.9531\n",
            "Epoch 80/100\n",
            "7/7 [==============================] - 1s 104ms/step - loss: 0.9481 - accuracy: 0.9577\n",
            "Epoch 81/100\n",
            "7/7 [==============================] - 1s 108ms/step - loss: 0.9332 - accuracy: 0.9624\n",
            "Epoch 82/100\n",
            "7/7 [==============================] - 0s 67ms/step - loss: 0.9094 - accuracy: 0.9624\n",
            "Epoch 83/100\n",
            "7/7 [==============================] - 1s 77ms/step - loss: 0.8912 - accuracy: 0.9671\n",
            "Epoch 84/100\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.8793 - accuracy: 0.9624\n",
            "Epoch 85/100\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.8505 - accuracy: 0.9718\n",
            "Epoch 86/100\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.8375 - accuracy: 0.9718\n",
            "Epoch 87/100\n",
            "7/7 [==============================] - 0s 61ms/step - loss: 0.8189 - accuracy: 0.9671\n",
            "Epoch 88/100\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.8002 - accuracy: 0.9718\n",
            "Epoch 89/100\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.7863 - accuracy: 0.9718\n",
            "Epoch 90/100\n",
            "7/7 [==============================] - 0s 70ms/step - loss: 0.7670 - accuracy: 0.9765\n",
            "Epoch 91/100\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.7479 - accuracy: 0.9765\n",
            "Epoch 92/100\n",
            "7/7 [==============================] - 1s 73ms/step - loss: 0.7315 - accuracy: 0.9765\n",
            "Epoch 93/100\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.7169 - accuracy: 0.9859\n",
            "Epoch 94/100\n",
            "7/7 [==============================] - 0s 65ms/step - loss: 0.7029 - accuracy: 0.9812\n",
            "Epoch 95/100\n",
            "7/7 [==============================] - 0s 62ms/step - loss: 0.6850 - accuracy: 0.9812\n",
            "Epoch 96/100\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.6699 - accuracy: 0.9906\n",
            "Epoch 97/100\n",
            "7/7 [==============================] - 0s 63ms/step - loss: 0.6535 - accuracy: 0.9906\n",
            "Epoch 98/100\n",
            "7/7 [==============================] - 0s 68ms/step - loss: 0.6416 - accuracy: 0.9906\n",
            "Epoch 99/100\n",
            "7/7 [==============================] - 0s 69ms/step - loss: 0.6274 - accuracy: 0.9859\n",
            "Epoch 100/100\n",
            "7/7 [==============================] - 0s 64ms/step - loss: 0.6122 - accuracy: 0.9906\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d67a9dc2f80>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text given a seed text\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "        predicted_index = np.argmax(predicted_probs)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Generate text\n",
        "first_word_index = 1\n",
        "while test_data[first_word_index] != ' ' and test_data[first_word_index] != '\\n':\n",
        "  first_word_index += 1\n",
        "test_paragraph = test_data[:first_word_index]\n",
        "print(f'test_paragraph = {test_paragraph}')\n",
        "generated_text = generate_text(test_paragraph, word_count_test, model, max_sequence_len)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbnu42Nork2E",
        "outputId": "4cd8e1fb-7676-471d-f2c5-2a7233364abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_paragraph =  the\n",
            " the dursleys had everything they wanted but they also had a secret and their greatest fear was that somebody would discover it they didn't think they could bear it if anyone found out about the potters mrs potter was mrs dursley's sister but they hadn't met for several years in fact mrs dursley pretended she didn't have a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate accuracy by char\n",
        "print(f'len(test_data) = {len(test_data)}')\n",
        "print(f'len(generated_text) = {len(generated_text)}\\n')\n",
        "min_len = min(len(test_data), len(generated_text))\n",
        "\n",
        "print(f'test_data = {test_data}\\n')\n",
        "print(f'generated_text = {generated_text}')\n",
        "print(generated_text)\n",
        "match_count = 0\n",
        "for i in range(min_len):\n",
        "  if test_data[i] == generated_text[i]:\n",
        "    match_count += 1\n",
        "accuracy = match_count / min_len\n",
        "print(f'accuracy = {round(accuracy * 100, 2)}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAEyjlCz8jDT",
        "outputId": "7b786aec-aad5-4622-881c-dda221cb2b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(test_data) = 309\n",
            "len(generated_text) = 324\n",
            "\n",
            "test_data =  the dursleys shuddered to think what the neighbors would say if the potters arrived in the street. the dursleys knew that the potters had a small son, too, but they had never even seen him. this boy was another good reason for keeping the potters away; they didn't want dudley mixing with a child like that. \n",
            "\n",
            "generated_text =  the dursleys had everything they wanted but they also had a secret and their greatest fear was that somebody would discover it they didn't think they could bear it if anyone found out about the potters mrs potter was mrs dursley's sister but they hadn't met for several years in fact mrs dursley pretended she didn't have a\n",
            " the dursleys had everything they wanted but they also had a secret and their greatest fear was that somebody would discover it they didn't think they could bear it if anyone found out about the potters mrs potter was mrs dursley's sister but they hadn't met for several years in fact mrs dursley pretended she didn't have a\n",
            "accuracy = 11.65%\n"
          ]
        }
      ]
    }
  ]
}